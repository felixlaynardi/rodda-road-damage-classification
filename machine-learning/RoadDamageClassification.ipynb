{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoadDamageClassification2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "09YIiL7O0-3U"
      },
      "source": [
        "# Import Library\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYaAnZ7B4bp",
        "outputId": "bad1b896-1c03-48d9-b0e4-983332bd528c"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IitzfBMS1K3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226a9f37-47c7-4657-ff6d-f18c322cc6b5"
      },
      "source": [
        "# Import datasets and pretrained model\n",
        "!mkdir /dataset\n",
        "\n",
        "!mkdir /pretrained_model\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://github.com/felixlaynardi/road-classification/raw/main/dataset/dt.zip \\\n",
        "    -O /dataset/dt.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "  -O /pretrained_model/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "\n",
        "# Extract datasets\n",
        "zip_path = '/dataset/dt.zip'\n",
        "ref = zipfile.ZipFile(zip_path, 'r')\n",
        "ref.extractall('/dataset')\n",
        "ref.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-02 13:25:59--  https://github.com/felixlaynardi/road-classification/raw/main/dataset/dt.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/felixlaynardi/road-classification/main/dataset/dt.zip [following]\n",
            "--2021-06-02 13:25:59--  https://raw.githubusercontent.com/felixlaynardi/road-classification/main/dataset/dt.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74799089 (71M) [application/zip]\n",
            "Saving to: ‘/dataset/dt.zip’\n",
            "\n",
            "/dataset/dt.zip     100%[===================>]  71.33M   127MB/s    in 0.6s    \n",
            "\n",
            "2021-06-02 13:26:03 (127 MB/s) - ‘/dataset/dt.zip’ saved [74799089/74799089]\n",
            "\n",
            "--2021-06-02 13:26:03--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 108.177.15.128, 173.194.76.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/pretrained_model/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/pretrained_model/i 100%[===================>]  83.84M  32.1MB/s    in 2.6s    \n",
            "\n",
            "2021-06-02 13:26:06 (32.1 MB/s) - ‘/pretrained_model/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf4hlSEYQ-hr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3855f132-82ea-4d80-be08-2159c609138c"
      },
      "source": [
        "# Get dataset size on graph\n",
        " \n",
        "left = [1, 2, 3]\n",
        "height = [len(os.listdir('/dataset/Crack')), len(os.listdir('/dataset/Pothole')), len(os.listdir('/dataset/Good'))]\n",
        "  \n",
        "tick_label = ['Crack', 'Potholes', 'Good']\n",
        "  \n",
        "plt.bar(left, height, tick_label = tick_label, width = 0.8, color = ['red', 'green', 'blue'])\n",
        "  \n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Image Amount')\n",
        "plt.title('Dataset')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d94Rkchd1cKV"
      },
      "source": [
        "# Put datasets into list for splitting training and test set\n",
        "data_dir = '/dataset'\n",
        "\n",
        "labels = [\"Crack\", \"Pothole\", \"Good\"]\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "# Read and resize image\n",
        "for label in labels:\n",
        "    data = os.path.join(data_dir,label)\n",
        "    for image in os.listdir(data):\n",
        "        try:\n",
        "            im = cv2.imread(os.path.join(data,image),cv2.IMREAD_COLOR)\n",
        "            im = cv2.resize(im,(224,224))\n",
        "            \n",
        "            x.append(im)\n",
        "            y.append(labels.index(label))\n",
        "            \n",
        "            \n",
        "        except Exception as e:\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNo2ghiY10WA",
        "outputId": "fb632206-04b5-4578-ae00-5d1b0220cb65"
      },
      "source": [
        "# Get size of data and labels\n",
        "len(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8puTKBA2yOD",
        "outputId": "98aa25d4-0452-43e1-dd15-6dacbf6064a2"
      },
      "source": [
        "len(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuKeymNi13p2"
      },
      "source": [
        "# Put list of datasets into numpy\n",
        "np.unique(y)\n",
        "np.unique(x)\n",
        "\n",
        "x = np.array(x)/255.0\n",
        "y = np.array(y)\n",
        "\n",
        "x.shape\n",
        "y.shape\n",
        "\n",
        "# Reshape numpy array\n",
        "x = x.reshape(-1, 224, 224, 3)\n",
        "x.shape\n",
        "y = y.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxv-eiiM1902"
      },
      "source": [
        "# Set y into one hot encoding\n",
        "y = to_categorical(y,3,)\n",
        "\n",
        "# Data augmentation\n",
        "datagenerator = ImageDataGenerator(\n",
        "  fill_mode= 'nearest',\n",
        "  horizontal_flip=False,\n",
        "  vertical_flip=False,\n",
        "  shear_range=0.1,\n",
        "  zoom_range = 0.1,\n",
        "  width_shift_range=0.2,\n",
        "  height_shift_range=0.2\n",
        ")\n",
        "datagenerator.fit(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChmVuQKc2BBt"
      },
      "source": [
        "# Split dataset into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnjX34YsX8L7"
      },
      "source": [
        "path_inception = f\"/pretrained_model/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "local_weights_file = path_inception\n",
        "\n",
        "pre_trained_model = InceptionV3(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights=None\n",
        ")\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# Make all the layers in the pre-trained model non-trainable\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "# Print the model summary\n",
        "# pre_trained_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFqhcvsvYbOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfe3da1-ce0a-472b-e217-2e4d1bc3883a"
      },
      "source": [
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "last_output = last_layer.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 12, 12, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQZsr-dS2BuY"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.3)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(3, activation='softmax')(x) \n",
        "\n",
        "model = Model(pre_trained_model.input, x) \n",
        "\n",
        "# Set RMSProp optimizer\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSWBMS_Y2K00",
        "outputId": "a43f0467-c5ad-4322-924f-c2e21df17c6b"
      },
      "source": [
        "# Set training callback\n",
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=4,restore_best_weights=True)\n",
        "\n",
        "# Train model in .. epochs\n",
        "model.fit(datagenerator.flow(x_train, y_train, batch_size=32,shuffle=True), epochs=20, validation_data=datagenerator.flow(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "38/38 [==============================] - 57s 685ms/step - loss: 1.0541 - accuracy: 0.7871 - val_loss: 0.1633 - val_accuracy: 0.9504\n",
            "Epoch 2/20\n",
            "38/38 [==============================] - 22s 583ms/step - loss: 0.2141 - accuracy: 0.9155 - val_loss: 0.1635 - val_accuracy: 0.9355\n",
            "Epoch 3/20\n",
            "38/38 [==============================] - 22s 584ms/step - loss: 0.1148 - accuracy: 0.9594 - val_loss: 0.1380 - val_accuracy: 0.9454\n",
            "Epoch 4/20\n",
            "38/38 [==============================] - 22s 584ms/step - loss: 0.1482 - accuracy: 0.9486 - val_loss: 0.0545 - val_accuracy: 0.9752\n",
            "Epoch 5/20\n",
            "38/38 [==============================] - 22s 586ms/step - loss: 0.1397 - accuracy: 0.9586 - val_loss: 0.0544 - val_accuracy: 0.9727\n",
            "Epoch 6/20\n",
            "38/38 [==============================] - 22s 589ms/step - loss: 0.0982 - accuracy: 0.9693 - val_loss: 0.0467 - val_accuracy: 0.9752\n",
            "Epoch 7/20\n",
            "38/38 [==============================] - 22s 586ms/step - loss: 0.0836 - accuracy: 0.9751 - val_loss: 0.0270 - val_accuracy: 0.9901\n",
            "Epoch 8/20\n",
            "38/38 [==============================] - 22s 586ms/step - loss: 0.0802 - accuracy: 0.9785 - val_loss: 0.1129 - val_accuracy: 0.9529\n",
            "Epoch 9/20\n",
            "38/38 [==============================] - 22s 585ms/step - loss: 0.0588 - accuracy: 0.9859 - val_loss: 0.0231 - val_accuracy: 0.9901\n",
            "Epoch 10/20\n",
            "38/38 [==============================] - 22s 588ms/step - loss: 0.0682 - accuracy: 0.9785 - val_loss: 0.1869 - val_accuracy: 0.9479\n",
            "Epoch 11/20\n",
            "38/38 [==============================] - 22s 584ms/step - loss: 0.0726 - accuracy: 0.9785 - val_loss: 0.0676 - val_accuracy: 0.9801\n",
            "Epoch 12/20\n",
            "38/38 [==============================] - 22s 585ms/step - loss: 0.0493 - accuracy: 0.9851 - val_loss: 0.2008 - val_accuracy: 0.9504\n",
            "Epoch 13/20\n",
            "38/38 [==============================] - 22s 585ms/step - loss: 0.0540 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9876\n",
            "Epoch 14/20\n",
            "38/38 [==============================] - 22s 587ms/step - loss: 0.0469 - accuracy: 0.9892 - val_loss: 0.0332 - val_accuracy: 0.9926\n",
            "Epoch 15/20\n",
            "38/38 [==============================] - 22s 586ms/step - loss: 0.0385 - accuracy: 0.9909 - val_loss: 0.0275 - val_accuracy: 0.9926\n",
            "Epoch 16/20\n",
            "38/38 [==============================] - 22s 585ms/step - loss: 0.0862 - accuracy: 0.9843 - val_loss: 0.0532 - val_accuracy: 0.9801\n",
            "Epoch 17/20\n",
            "38/38 [==============================] - 22s 584ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0654 - val_accuracy: 0.9801\n",
            "Epoch 18/20\n",
            "38/38 [==============================] - 22s 585ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0128 - val_accuracy: 0.9975\n",
            "Epoch 19/20\n",
            "38/38 [==============================] - 22s 585ms/step - loss: 0.0237 - accuracy: 0.9892 - val_loss: 0.0135 - val_accuracy: 0.9926\n",
            "Epoch 20/20\n",
            "38/38 [==============================] - 22s 583ms/step - loss: 0.0436 - accuracy: 0.9859 - val_loss: 0.0240 - val_accuracy: 0.9901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fef0f92e310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8cmjWSn2UpF"
      },
      "source": [
        "# Convert inputted image into array\n",
        "from PIL import Image\n",
        "def convert_to_array(img):\n",
        "    im = cv2.imread(img)\n",
        "    img = Image.fromarray(im, 'RGB')\n",
        "    image = img.resize((224, 224))\n",
        "    return np.array(image)\n",
        "\n",
        "# Set label into Crack and Pothole\n",
        "def get_profile_name(label):\n",
        "    if label==0:\n",
        "        return \"Crack\"\n",
        "    if label==1:\n",
        "        return \"Pothole\"\n",
        "    if label==2:\n",
        "        return \"Good\"\n",
        "\n",
        "# Predict function\n",
        "def predict_profile(file):\n",
        "    print(\"Predicting .................................\")\n",
        "    ar = convert_to_array(file)\n",
        "    ar = ar/255\n",
        "    a = []\n",
        "    a.append(ar)\n",
        "    a = np.array(a)\n",
        "    score = model.predict(a,verbose=1)\n",
        "    print(score)\n",
        "    label_index = np.argmax(score)\n",
        "    print(label_index)\n",
        "    acc = np.max(score)\n",
        "    profile = get_profile_name(label_index)\n",
        "    print(profile)\n",
        "    print(\"The predicted profile is a \" + profile + \" with accuracy =    \" + str(acc))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COGZILyZ2Xj0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "0f92f07c-b29f-4aef-d346-7cb49786ff06"
      },
      "source": [
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  # predicting images\n",
        "  path = fn\n",
        "  predict_profile(path)\n",
        "  # img = image.load_img(path, target_size=(224, 224))\n",
        "  # x = image.img_to_array(img)\n",
        "  # x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  # images = np.vstack([x])\n",
        "  # classes = model.predict(images, batch_size=10)\n",
        "  # print(fn)\n",
        "  \n",
        "  # print(classes[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7eff3ad-42a8-45f8-ac5b-5da62ea75120\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7eff3ad-42a8-45f8-ac5b-5da62ea75120\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 77747.jpg to 77747.jpg\n",
            "Predicting .................................\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "[[7.9498726e-01 2.0501269e-01 4.3668502e-17]]\n",
            "0\n",
            "Crack\n",
            "The predicted profile is a Crack with accuracy =    0.79498726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2lQWkBfUcy2"
      },
      "source": [
        "model.save('pothole_classification.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xra7Jz5oPLD5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}